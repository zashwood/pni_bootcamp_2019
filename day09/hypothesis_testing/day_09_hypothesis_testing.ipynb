{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPORTS\n",
    "import numpy as np\n",
    "import scipy.stats as stats # has function pearsonr(x,y)\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_boston # We will work with the Boston housing data in some of what follows later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: T-tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will apply a t-test to data generated from normal distributions with different means.  We will investigate if the test is able to detect that the data is generated from two different distributions as a function of both (1) the difference in means for the generating distribution and (2) as a function of the number of samples.   We will also explore applying a t-test to data that was not generated from a normal distribution, and instead was generated from a chi-squared distribution and investigate the same things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we generate data from two normal distributions with the same standard deviation\n",
    "# We would like any statistical test that we run to be able to tell us that the underlying \n",
    "# population means responsible for generating x1 and x2 are different\n",
    "mean1 = 1\n",
    "true_diff_in_means = 0.2\n",
    "mean2 = mean1 + true_diff_in_means\n",
    "\n",
    "x1 = np.random.standard_normal((100,)) + mean1\n",
    "x2 = np.random.standard_normal((100,)) + mean2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the sampled data\n",
    "plt.hist(x1, label = \"x1\")\n",
    "plt.hist(x2, label = \"x2\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"Number of datapoints with value x\")\n",
    "plt.xlabel(\"x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now run a t-test: is the null hypothesis rejected?\n",
    "returned_stat, p_val = stats.ttest_ind(x1, x2)\n",
    "print(p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: explore how the rejection of the null hypothesis varies as a function of \n",
    "# the number of samples and as a function of the difference in generative means. \n",
    "# Produce an image with number of samples on the x-axis and difference in means on the y-axis \n",
    "# and color a pixel based on whether the null hypothesis was rejected for the given value \n",
    "# of the difference and number of samples. It might be good to keep mean1 fixed, and then \n",
    "# just alter the true_diff_in_means parameter in case the rejection rate is affected by the \n",
    "# size of mean1.  Note: the function plt.imshow() is useful for showing images.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now modify the distribution that we sample x1 and x2 from, and instead sample from \n",
    "# the chi-squared distribution \n",
    "mean1 = 53\n",
    "true_diff_in_means = 2\n",
    "mean2 = mean1 + true_diff_in_means\n",
    "x1 = stats.chi2.rvs(df = mean1, size=100)\n",
    "x2 = stats.chi2.rvs(df = mean2, size=100)\n",
    "\n",
    "plt.hist(x1, label = \"x1\")\n",
    "plt.hist(x2, label = \"x2\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"Number of datapoints with value x\")\n",
    "plt.xlabel(\"x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now run a t-test: is the null hypothesis rejected?\n",
    "returned_stat, p_val = stats.ttest_ind(x1, x2)\n",
    "print(p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: do as before, and explore how the rejection of the null hypothesis varies as a \n",
    "# function of the number of samples and as a function of the difference in generative means \n",
    "# but now for data generated from the chi-squared distribution.\n",
    "# Produce an image with number of samples on the x-axis and difference in means on the y-axis \n",
    "# and color a pixel based on whether the null hypothesis was rejected for the given value \n",
    "# of the difference and number of samples. It might be good to keep mean1 fixed, and then \n",
    "# just alter the true_diff_in_means parameter in case the rejection rate is affected by the \n",
    "# size of mean1.  Note: the function plt.imshow() is useful for showing images.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: Does the t-test always detect a difference in means when the generative distribution has a difference in means?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Permutation test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In what follows, we would like to investigate the relationship between the per capita crime of a town and  the median value of house price in the town.  We will work with the Boston housing dataset that is documented here (https://scikit-learn.org/stable/datasets/index.html#boston-dataset).  You will write a function to implement a permutation test in order to determine whether the measured correlation value is statistically significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### HELPER FUNCTION\n",
    "def compute_corr(x,y):\n",
    "    \"\"\"\n",
    "    Function to compute the correlation between two vectors, x and y\n",
    "    \n",
    "    :param x: vector of values\n",
    "    :param y: vector of values\n",
    "    :return: Pearson correlation between vectors x and y\n",
    "    \"\"\"\n",
    "    mean_x = np.mean(x)\n",
    "    mean_y = np.mean(y)\n",
    "    var_x = np.var(x)\n",
    "    var_y = np.var(y)\n",
    "    x_recentered = x - mean_x\n",
    "    y_recentered = y - mean_y\n",
    "    corr = 1/x.size * np.dot(x_recentered, y_recentered) / np.sqrt(var_x * var_y)\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load the data that we will work with: the Boston housing dataset.\n",
    "# This is a commonly used dataset in Machine Learning classes.  You can read more about this\n",
    "# dataset and its attributes here: \n",
    "# https://scikit-learn.org/stable/datasets/index.html#boston-dataset\n",
    "boston = load_boston()\n",
    "print(boston.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the list of predictors we have for predicting house prices:\n",
    "boston.feature_names  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's extract out the per capita crime by town:\n",
    "crime = boston.data[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Median value of owner-occupied homes in $1000’s for a town\n",
    "house_prices = boston.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now plot this data:\n",
    "plt.scatter(crime, house_prices)\n",
    "plt.ylabel(\"House price\", fontsize = 15)\n",
    "plt.xlabel(\"Crime per capita\", fontsize = 15)\n",
    "plt.title(\"Boston house price dataset\", fontsize = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use scipy's stats package in order to calculate the correlation\n",
    "# and compare the value they calculate to the one we calculate with our \"compute_corr\" func\n",
    "r_true, pval = stats.pearsonr(crime,house_prices)\n",
    "\n",
    "print(r_true)\n",
    "\n",
    "r_computed = compute_corr(crime,house_prices)\n",
    "print(r_computed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Write a function to shuffle the house price values associated with each per capita\n",
    "# crime value\n",
    "\n",
    "def shuffle_data(house_prices):\n",
    "    \"\"\"\n",
    "    Return a vector of shuffled house price data\n",
    "    \n",
    "    :param house_prices: true vector of house price values\n",
    "    :return shuffled_prices: vector of shuffled house price values\n",
    "    \"\"\"\n",
    "    # TODO: Fill this in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Implement a permutation test\n",
    "N_permutations = 1000\n",
    "corr_vals = []\n",
    "for i in range(N_permutations):\n",
    "    # TODO: Fill this in; append a correlation value, corr_val, to the corr_vals list\n",
    "    corr_vals.append(corr_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of correlation values and add a vertical line to show the true correlation\n",
    "# value:\n",
    "plt.hist(corr_vals, bins = 100)\n",
    "plt.axvline(x=r_computed, color = 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: write a function to calculate the P value given the null distribution you distributed\n",
    "## from shuffling the data, and the true correlation value, r_computed\n",
    "def calculate_p_value(corr_vals, r_computed):\n",
    "    \"\"\"\n",
    "    Given the true correlation value for the data, r_computed, calculate a p-value based on \n",
    "    correlation values obtained with shuffling\n",
    "    \n",
    "    :param corr_vals: list of correlation values obtained by shuffling the data\n",
    "    :param r_computed: true correlation value obtained from the data\n",
    "    :return p_val: p value\n",
    "    \"\"\"\n",
    "    ## TODO: FIll this in\n",
    "    return p_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "    1.  Compare the p value that you obtained with your implementation of the permutation test with that that scipy's stats package would return (run ``r_true, pval = stats.pearsonr(crime,house_prices)`` to get scipy's stats p-value).  If there are differences, why do you think this is?\n",
    "    2.  What is the effect of increasing/decreasing the number of permutations that you carry out?  How does the shape of the null distribution change?  How does the retrieved p-value change?  (Try a few values; e.g. 10, 100, 1000, 10000)\n",
    "    3.  State explicitly what the null hypothesis and the alternative hypothesis are when you are carrying out the permutation test on this data\n",
    "    4.  Are you able to reject the null hypothesis?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
